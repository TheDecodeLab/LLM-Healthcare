{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f306c6a6-1548-42ba-add9-321009b65a33",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc45fe7-e3e8-4191-ab04-016e7ee6b68e",
   "metadata": {},
   "source": [
    "##### Import pandas for data manipulation and Counter from collections for counting elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096d869-60dc-4f95-8d53-f934713abb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32be3e4-5bdd-4899-a693-4f9a100dd35d",
   "metadata": {},
   "source": [
    "# Read the CSV file\n",
    "##### Load the data from 'data/simpsons_script_lines.zip' assuming it's a zipped CSV file\n",
    "##### Use 'compression='zip'' to handle the zipped format\n",
    "##### Display the first two rows using head(2) to get a glimpse of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04316c-d86c-4a80-aec3-c864ac919384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv('simpsons_script_lines.zip',compression='zip')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cdd43-ece9-4395-b0ad-ffb8849e8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the raw text column and create word frequency\n",
    "# Get the 'raw_text' column from the DataFrame\n",
    "raw_text = data[\"raw_text\"]\n",
    "\n",
    "# Tokenize the text into words by splitting each text entry\n",
    "words = []\n",
    "for text in raw_text:\n",
    "    words.extend(text.split())\n",
    "\n",
    "# Use Counter to calculate the frequency of each word\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Print the 10 most common words\n",
    "print(word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df679f5-ea8c-4d0d-84d5-02757a3037dc",
   "metadata": {},
   "source": [
    "## Create a WordCloud visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fba094-51d3-41b7-88b9-21b561d61e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define WordCloud parameters: width, height, and background color\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "# Generate the word cloud based on word frequencies\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc5493-bf85-464b-95e1-5b3861875639",
   "metadata": {},
   "source": [
    "##### Analyze frequency of characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8fab1-064e-4486-885a-d343718e8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['raw_character_text'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6d789-d75c-436f-a852-c00c0ff5f523",
   "metadata": {},
   "source": [
    "##### Analyze word frequency for specific characters\n",
    "##### Loop through characters ('Homer Simpson', 'Marge Simpson', etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ef008-21f6-46a0-a4ff-e0d3ef29c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in ['Homer Simpson','Marge Simpson','Bart Simpson','Lisa Simpson']:\n",
    "\n",
    "    raw_text = data[data['raw_character_text']==ch][\"raw_text\"]\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = []\n",
    "    for text in raw_text:\n",
    "        words.extend(text.split())\n",
    "    \n",
    "    # Calculate word frequency\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Print the most common words\n",
    "    print(ch)\n",
    "    print(word_freq.most_common(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bbfe4-61b1-4e37-8713-f0bb1f4e9a9f",
   "metadata": {},
   "source": [
    "##### Perform word frequency analysis with stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d3cdd-e3a9-469e-8d7a-b8474dc4bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import corpus\n",
    "stop_words = corpus.stopwords.words('english')\n",
    "# Print the first 10 stop words\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06291e-7da1-4724-a7d2-d8b6ee6e8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words and add character names (optional customization)\n",
    "stop_words = corpus.stopwords.words('english')+['Homer','Marge','Bart','Lisa','Simpson']\n",
    "# Convert all stop words to lowercase\n",
    "stop_words = [word.lower() for word in stop_words]\n",
    "\n",
    "# Repeat the analysis for each character, filtering out stop words\n",
    "for ch in ['Homer Simpson','Marge Simpson','Bart Simpson','Lisa Simpson']:\n",
    "\n",
    "    raw_text = data[data['raw_character_text']==ch][\"raw_text\"]\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = []\n",
    "    for text in raw_text:\n",
    "        words.extend(text.split())\n",
    "\n",
    "    # Convert words to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Filter out stop words using list comprehension\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Calculate word frequency\n",
    "    word_freq = Counter(filtered_words)\n",
    "    \n",
    "    # Print the most common words\n",
    "    print(ch)\n",
    "    print(word_freq.most_common(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804c4e5-30b6-4424-9ff8-4640c48031dd",
   "metadata": {},
   "source": [
    "### Find occurrences of \"simpson\" and variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44bd62-4f7c-4e76-a6f4-ffa442efc0b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "pattern = r\"simpson.*\"\n",
    "\n",
    "# Find all matches in the text\n",
    "np.unique( [re.findall(pattern, word, re.IGNORECASE) for word in words if 'simpson' in word] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6566832-e0d8-4f83-922f-4bd02eb011a0",
   "metadata": {},
   "source": [
    "# Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90a42f-5a40-4e90-b62b-a6b59834be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input signal and kernel\n",
    "input_signal = np.array([1, 2, 3, 4, 5])\n",
    "kernel = np.array([0, 1, 0])\n",
    "\n",
    "# Perform convolution\n",
    "output = np.convolve(input_signal, kernel, mode='valid')\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93c57c-55a2-491e-977b-8638f7ed7914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "eda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
